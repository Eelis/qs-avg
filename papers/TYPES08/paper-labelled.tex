\documentclass[runningheads]{llncs}

\newcommand{\eelis}{Eelis van der Weegen}
\newcommand{\james}{James McKinna}

\newcommand{\theauthors}{\eelis\ and \james}
\newcommand{\thetitle}{A Machine-checked Proof of the Average-case Complexity of Quicksort in Coq}

\usepackage{color}
\usepackage{amsmath}
\usepackage[colorlinks,pdftitle=\thetitle,pdfauthor=\theauthors]{hyperref}

\spnewtheorem*{propo}{Proposition}{\bfseries}{\rmfamily}

%include polycode.fmt

%format pipe = "\mathopen{|}"

%format times = "\times"

%format . = "."
%format forall = "\forall"
%format Fixpoint = "\mathbf{Fixpoint}"
%format Definition = "\mathbf{Definition}"
%format Eval = "\mathbf{Eval}"
%format Notation = "\mathbf{Notation}"
%format Lemma = "\mathbf{Lemma}"
%format Proof = "\mathbf{Proof}"
%format Defined = "\mathbf{Defined}"
%format Function = "\mathbf{Function}"
%format Proposition = "\mathbf{Proposition}"
%format Record = "\mathbf{Record}"
%format Program = "\mathbf{Program}"
%format Inductive = "\mathbf{Inductive}"
%format Theorem = "\mathbf{Theorem}"
%format Let = "\mathbf{Let}"
%format Variable = "\mathbf{Variable}"
%format Variables = "\mathbf{Variables}"
%format measure = "\mathbf{measure}"

%format prod = "\hspace{-1mm}\times\hspace{-1mm} "
%format sp = "\ "

%format == = "= "
%format /= = "\ne "
  % Without these, lhs2tex uses three-line equal signs.

%format fcmp = "\circ"
%format match = "\mathbf{match}"
%format with = "\mathbf{with}"
%format end = "\mathbf{end}"
%format fun = "\lambda\hspace{-1mm} "
%format nat = "\mathbb{N}"
%format R = "\mathbb{R}"
%format exists = "\exists "
%format log2ceil = "\log_{2}\hspace{-0.5mm}"
%format over = "\mathbf{over}"
%format INR (a) = a
%format /\ = "\wedge "
%format \/ = "\vee"
%format <-> = "\leftrightarrow "
%format MonoidMonadTrans.M = "\Varid{MMT}"
%format NatAddMonoid = (nat, 0, +)

%format { = "\hspace{0.9mm}\{"
  % so that parameters declared with {...} are spaced like those declared with (...)

%format sqrd (a) = (a)"^2"

%format `In` = "\in"
%format `NotIn` = "\notin"

%format C_MMT
%format ret_MMT
%format bind_MMT

%format NeTreeMonad = "M_\Varid{ne\_tree}"
%format M_ne_tree = "M_\Varid{ne\_tree}"
%format bind_ne_tree = "\Varid{bind}_\Varid{ne\_tree}"
%format ret_ne_tree = "\Varid{ret}_\Varid{ne\_tree}"
%format pick_ne_tree = "\Varid{pick}_\Varid{ne\_tree}"

%format Monoid_U
%format cmp_U
%format pick_U
%format partition_U
%format qs_U
%format U.qs = "\Varid{qs}_U"

%format M_NDP
%format cmp_NDP
%format qs_NDP

%format le_SP
%format ret_SP
%format bind_SP
%format qs_SP

% todo:
% - check that names are spelled and encoded right in bibliography
% - decide whether to hide, ignore, or explain R/N differences. explaining costs precious space. ignoring is "begging the question". hiding is naughty.
% - have a sentence somewhere hinting at the complications caused by duplicate elements (e.g., by pointing out simplifications that hold only with the assumption of no duplicate elements).

\def\commentbegin{\quad\{\ }
\def\commentend{\}}

\def\typeset{}

\usepackage{macros}

\arrayhs % i.e. no page breaks in code blocks

\begin{document}

\nocite{*}

\title{\thetitle}

\authorrunning{\em}
\titlerunning{\em}

\author{\eelis\thanks{Research carried out by the first author as part of the Master's programme in ``Foundations''} and \james \\
eelis@@eelis.net, james.mckinna@@cs.ru.nl}

\institute{Department of Computer Science, University of Nijmegen \\
P.O. Box 9010, 6500 GL Nijmegen, The Netherlands}

\maketitle

\begin{abstract}
  \input{abstract.ltx}
\end{abstract}

\section{Introduction}
\label{intro}

Proofs of the $O(n \log n)$ average-case complexity of Quicksort \cite{HoareQuick} are included in many textbooks on computational complexity \cite[for example]{introtoalgos}.
This paper documents what the authors believe to be the first fully formal machine-checked version of such a proof, developed using the Coq proof assistant \cite{coq}.

The formalisation is based on the ``paper proof'' in \cite{introtoalgos}, which consists of three parts. The first part shows that the total number of comparisons performed by the algorithm (the usual complexity metric for sorting algorithms) can be written as a sum of expected comparison counts for individual pairs of input list elements. The second part derives from the algorithm a specific formula for this expectation. The third and last part employs some analysis involving the harmonic series to derive the $O(n \log n)$ bound from the sum-of-expectations.

Of these three parts, only the first two involve the actual algorithm itself---the third part is strictly numerical. While the original proof provides a thorough treatment of the third part, its treatment of the first two parts is informal in two major ways.

First, it never actually justifies anything in terms of the algorithm's formal semantics. Indeed, it does not even formally define the algorithm in the first place, relying instead on assertions which are taken to be intuitively true. While this practice is common and perfectly reasonable for paper proofs intended for human consumption, it is a luxury we do not afford ourselves.

Second, the original proof (implicitly) assumes that the input list does not contain any duplicate elements, which significantly simplifies its derivation of the formula for the expected comparison count for pairs of individual input list elements. We do not make this assumption.
  % todo: perhaps bring this up later.

The key to giving a proper formal treatment of both of these aspects lies in the use of an appropriate representation of the algorithm, capable of capturing its computational behavior---specifically, its use of comparisons---in a way suitable for subsequent formal reasoning. 
The appproach we take is to consider such operation-counting as a \emph{side effect}, and to use the general framework of \emph{monads} for representing side-effecting computation in pure functional languages. 
The representation we use takes the form of a shallow monadic embedding, in which the algorithm, here Quicksort, is written as a monadically expressed functional program. This definition is then instantiated with refinements of operation-counting monads to make the comparison count observable.

The embedding is introduced in section \ref{embed}, where we demonstrate its use by first giving a simple deterministic monadic Quicksort definition, and then instantiating it with a simple operation counting monad that lets us prove its quadratic worst-case complexity.

For the purposes of the more complex average-case theorem, we then give (in section \ref{nondetexpec}) a potentially-nondeterministic monadic Quicksort definition, and compose a monad that combines operation counting with nondeterminism, supporting a formal definition of the notion of the \emph{expected} comparison count, with which we state the main theorem in section \ref{statement}.

The next two sections detail the actual formalised proof. Section \ref{reduction} corresponds to the first part in the original proof described above, and shows how the main theorem can be split up into a lemma (stated in terms of another specialized monad) giving a formula for the expected comparison count for individual pairs of input element, and a strictly numerical part. Since we were able to fairly directly transcribe the latter part from the paper proof, using the existing real number theory in the Coq standard library with few complications and additions, we omit a discussion of it in this paper and refer the interested reader to the paper proof.

Section \ref{finishing} finishes the proof by proving the lemma about the expected comparison count for individual input list elements. Since this is the part where the original proof omits the most detail, and makes the assumption regarding duplicate elements, and where we really have to reason in detail about the behavior of the algorithm, it is by far the most involved part of the formalisation.

Section \ref{conclusion} ends with conclusions and final remarks.

The Coq source files containing the entire formalisation can be downloaded from \url{http://www.eelis.net/research/quicksort/}. The Coq version used is 8.2 beta 2.

\paragraph{Related work}

In his Ph.D thesis \cite{hurd}, Hurd presents an approach to formal analysis
of probabilistic programs based on a comprehensive formalisation of
measure-theoretic constructions of probability spaces, representing
probabilistic programs using a state-transforming monad in which bits from an
infinite supply of random bits may be consumed. He even mentions the problem
of proving the average-case complexity of Quicksort, but leaves it for future work.

In \cite{mohringaudebaud}, Audebaud and Paulin-Mohring describe a different monadic approach in which programs are interpreted directly as measures representing probability distributions. A set of axiomatic rules is defined for estimating the probability that programs interpreted this way satisfy certain properties.

Compared to these approaches, our infrastructure for reasoning about nondeterministic programs is rather less ambitious, in that we only consider finite expectation based on na{\"\i}ve counting probability, using a monad for nondeterminism which correctly supports weighted expectation. In particular, we do not need to reason explicitly with probability distributions.

A completely different approach to type-theoretic analysis of computational complexity is to devise a special-purpose type theory in which the types of terms include some form of complexity guarantees. This is the approach taken in \cite{constable}, for example.

\section{A Shallow Monadic Embedding}
\label{embed}

As stated before, the key to giving a proper formal treatment of those parts of the proof for which the original contents itself with appeals to intuition, lies in the use of an appropriate representation of the algorithm. Indeed, we cannot even formally state the main theorem until we have both an algorithm definition and the means to denote its use of comparisons.

Since we are working in Coq, we already have at our disposal a full functional
programming language, in the form of Coq's pCIC \cite{coq'art}. However, just
writing the algorithm as an ordinary Coq function would not let us observe its
use of comparisons. 
We can however see comparison counting as a \emph{side effect}. As is well
known and standard practice in functional languages such as Haskell, side
effects can be represented using \emph{monads}: a side-effecting function |f|
from |A| to |B| is represented as a function |A -> M B| where |M| is a type
constructor encapsulating the side effects. ``Identity'' and ``composition''
for such functions are given by |ret| (for ``return'') of type |A -> M A| and
|bind| (infix: |>>=|) of type |M A -> (A -> M B) -> M B| satisfying certain
identities (the \emph{monad laws}). For a general introduction to monadic
programming and monad laws, see \cite{wadler93monads}. Furthermore, we use
Haskell's ``do-notation'',
\begin{code}
 Notation "x <- y ; z" := (bind y (fun x: _ => z)) 
\end{code}
and freely use standard monadic functions such as:
\begin{code}
 liftM: forall (M: Monad) (A B: Set), (A -> B) -> (M A -> M B)
 filterM: forall (M: Monad) (A: Set), (A -> M bool) -> list A -> M (list A)
\end{code}
Here, the Coq type |Monad| is a dependent record containing the (coercible) carrier of type |Set
-> Set|, along with the |bind| and |ret| operations, and proofs of the three monad laws. 
% We make those uses observable by writing the algorithm as a monadically
% expressed function, parameterizing it on both the monad itself and on the
% comparison operation. 

We now express Quicksort in this style, parameterizing it on both the monad
itself and on the comparison operation. A deterministic Quicksort that simply selects the head of the input list as its pivot element, and uses two simple filter passes to partition the input list, looks as follows:

\begin{code}
  Variables (M: Monad) (T: Set) (le: T -> T -> M bool).

  Definition gt (x y: T): M bool := liftM negb (le x y).

  Program Fixpoint qs (l: list T) {measure length l}: M (list T) :=
    match l with
    | nil => ret nil
    | pivot :: t =>
        lower <- filterM (gt pivot) t >>= qs;
        upper <- filterM (le pivot) t >>= qs;
        ret (lower ++ pivot :: upper)
    end.
\end{code}

We use Coq's |Program Fixpoint| facility \cite{subsetcoercions} to cope with Quicksort's non-structural recursion, specifying list length as an input measure function that is separately shown to strongly decrease for each recursive call. For this definition of |qs|, these proof obligations are trivial enough for Coq to prove mostly by itself.

For recursive functions defined this way, Coq does not automatically define corresponding induction principles matching the recursive call structure. Hence, for this |qs| definition as well as the one we will introduce in section \ref{nondetexpec}, we had to define these induction principles manually. To make their use as convenient as possible, we further customized and specialized them to take advantage of specific monad properties. We will omit further discussion of these issues in this paper, and will henceforth simply say: ``by induction on |qs|, ...''.

By instantiating the above definitions with the right monad, we can transparently insert comparison-counting instrumentation into the algorithm, which will prove to be sufficient to let us reason about its complexity. But before we do so, let us note that if the above definitions are instead instantiated with the identity monad and an ordinary elementwise comparison on |T|, then the monadic scaffolding melts away, and the result is equivalent to an ordinary non-instrumented, non-monadic version, suitable for extraction and correctness proofs (which are included in the formalisation for completeness). This means that while we will instantiate the definitions with less trivial monads to support our complexity proofs, we can take some comfort in knowing that the object of those proofs is, in a very concrete sense, the actual Quicksort algorithm (as one would write it in a functional programming language), rather than some idealized model thereof.

For reasons that will become clear in later sections, we compose the monad we will instantiate the above definitions with using a monad transformer \cite{monadtrans} |MMT| (for ``monoid monad transformer''), which piggybacks a monoid onto an existing monad by pairing.

\begin{code}
  Variables (monoid: Monoid) (monad: Monad).

  Let C_MMT (T: Set): Set := monad (monoid prod T).

  Let ret_MMT (T: Set): T -> C_MMT T := ret fcmp pair (monoid_zero monoid).

  Let bind_MMT (A B: Set) (a: C_MMT A) (ab: A -> C_MMT B): C_MMT B :=
    x <- a; y <- ab (snd x); ret (monoid_mult monoid (fst x) (fst y), snd y).

  Definition MMT: Monad := Build_Monad C_MMT bind_MMT ret_MMT.
\end{code}
(In the interest of brevity, we omit proofs of the monad laws for |MMT| and all other monads defined in this paper. These proofs can all be found in the Coq code.)

We now use |MMT| to piggyback the additive monoid on |nat| onto the identity monad, and lift elementwise comparison into the resulting monad, which we call |SP| (for ``simply-profiled'').
\begin{code}
  Definition SP: Monad := MMT NatAddMonoid IdMonad.

  Definition le_SP (x y: nat): SP bool := (1, le x y).
\end{code}
When instantiated with this monad and comparison operation, |qs| produces the comparison count as part of its result.
\begin{code}
  Definition qs_SP := qs SP le_SP.

  Eval compute in qs_SP (3 :: 1 :: 0 :: 4 :: 5 :: 2 :: nil).
    = (16, 0 :: 1 :: 2 :: 3 :: 4 :: 5 :: nil)
\end{code}

%format `infixbind_SP` = >>="_{SP}"

Defining |cost| and |result| as the first and second projection, respectively, we trivially have identities such as |cost (ret_SP x) = 0|, |cost (le_SP x y) = 1|, and |cost (x `infixbind_SP` f) = cost x + cost (f (result x))|. This very modest amount of machinery is sufficient for a straightforward proof of Quicksort's quadratic worst-case complexity.

\begin{propo}|qs_worst: forall l, cost (qs_SP l) <= sqrd (length l).|\footnote{We do not use big-O notation for this simple statement, as it would only obfuscate. Big-O complexity is discussed in section \ref{statement}.}
\end{propo}

\begin{proof}
  The proof is by induction on |qs|. For |l==nil|, we have {|cost (qs_SP nil) = cost (ret nil) = 0 <= sqrd (length l)|}. For |l==h :: t|, the cost decomposes into
  \begin{code}
    cost (filter (le h) t) + cost (qs_SP (result (filter (le h) t))) +
    cost (filter (gt h) t) + cost (qs_SP (result (filter (gt h) t))) +
    cost (ret (result (qs_SP (result (filter (le h) t))) ++
     h :: result (qs_SP (result (filter (gt h) t))))).
  \end{code}
  The |filter| costs are easily proved (by induction on |t|) to be |length t| each. The cost of the final |ret| is 0 by definition. The induction hypothesis applies to the recursive |qs_SP| calls. Furthermore, by induction on |t|, we can easily prove
  \begin{code}
  length (result (filter (le h) t)) + length (result (filter (gt h) t)) <= length t,
  \end{code}
  because the two predicates filtered on are mutually exclusive. Abstracting the filter terms as |flt| and |flt'|, this leaves
  \begin{code}
    length flt + length flt' <= length t ->
      length t + sqrd (length flt) + length t + sqrd (length flt') + 0 <= sqrd (S (length t)),
  \end{code}
  which is true by elementary arithmetic. \qed
\end{proof}

We now extend the technique to prepare for the average-case complexity proof.

\section{Nondeterminism and Expected Values}
\label{nondetexpec}


The version of Quicksort used in the average-case proof in \cite{introtoalgos}
differs from the one presented in the last section in two ways. This is also
reflected in our formalisation.  

First, the definition of |qs| is modified to use a single three-way partition
pass, instead of two calls to |filter|, thus avoiding the pathological
quadratic behaviour which can arise when the input list does not consist of
distinct elements.

Second, and more significantly, we use \emph{nondeterministic} pivot
selection, thus avoiding the pathological quadratic behaviour from which
any deterministic pivot selection strategy inevitably suffers. While this
means that we have proved our result for a subtly different presentation of
Quicksort, this nevertheless follows the textbook treatment, in line with
common practice.

These two modifications together greatly simplify the formalisation, because
they remove the need to carefully track input distributions in order to show
that `good' inputs (for which the original deterministic version of the
algorithm performs well) sufficiently outnumber `bad' inputs (for which the
original version performs poorly). They further ensure that the $O(n \log n)$
average-case bound holds not just averaged over all possible input lists, but
for each individual input list as well. In particular, it means that once we
prove that the bound holds for an arbitrary input, the global bound
immediately follows.

This also means that for a key lemma near the end of our proof, we can use
straightforward induction over the algorithm's recursive call structure,
without having to show that given appropriately distributed inputs, the
partition step yields lists that are again appropriately distributed. Such
issues are a major technical concern in more ambitious approaches to
average-case complexity analysis \cite[for example]{modularcalculus} and to
the analysis of probabilistic algorithms.

The second modification is based on a new monad (again defined using |MMT|,
but this time transforming a nondeterminism monad) with which the new
definition can be instantiated, capturing the \emph{expected} comparison
count.

The first modification is relatively straightforward. Instead of calling
|filterM|, which uses a two-way comparison operation producing a monadic |bool|, we
define a function |partition|. It takes a three-way comparison operation producing a monadic |comparison|, which is an enumeration with values |Lt|, |Eq|, and |Gt|. We represent the resulting partitioning by a function of type |comparison -> list T| rather than a record or tuple type containing three lists, because in the actual formalisation, this saves us from having to constantly map |comparison| values to corresponding record field accessors or tuple projections. This is only a matter of minor convenience; a record or tuple could have been used instead without problems.

\begin{code}
  Variables (T: Set) (M: Monad) (cmp: T -> T -> M comparison).

  Fixpoint partition (t: T) (l: list T): M (comparison -> list T) :=
    match l with
    | nil => ret (const nil)
    | h :: l' =>
        c <- cmp h t; f <- partition t l';
        ret (fun c' => if c == c' then h :: f c' else f c')
    end.
\end{code}

Next, we redefine |qs| to use |partition|, and have it take as an additional parameter a |pick| operation, representing nondeterministic selection of an element of a non-empty list of choices. An |ne_list T| is a non-empty list of |T|'s, inductively defined in the obvious way. 

\begin{code}
  Variable pick: forall A: Set, ne_list A -> M A.

  Program Fixpoint qs (l: list T) {measure length l}: M (list T) :=
    match l with
    | nil => ret nil
    | _ =>
        i <- pick [0 ... length l - 1];
        let pivot := nth l i in
        part <- partition pivot (remove l i);
        low <- qs (part Lt);
        upp <- qs (part Gt);
        ret (low ++ pivot :: part Eq ++ upp)
    end.
\end{code}
The functions |nth| and |remove| select and remove the $n$th element of a list, respectively.

Note that the deterministic Quicksort definition in section \ref{embed} could also have been implemented with a |partition| pass instead, which might well have made the worst-case proof even simpler. We chose not to do this, in order to emphasize that the properties the average-case proof demands of the algorithm rule out the na{\"\i}ve but familiar implementation using |filter| passes.

Nondeterminism can now be emulated by instantiating these definitions with a suitable monad and |pick| operation. A deterministic, non-instrumented version can still be obtained, simply by using the identity monad and any deterministic |pick| operation, such as |head| or 'median-of-three' (not considered here).

Let us now consider what kind of nondeterminism monad would be suitable for reasoning about the expected value of a nondeterministic program like
\begin{code}
  x <- pick [0, 1]; if x = 0 then ret 0 else pick [1, 2].
\end{code}
When executed in the list monad (commonly used to emulate nondeterministic computation), this program
produces |[0, 1, 2]| as its list of possible outcomes. Unfortunately, the information that 0 is a more likely outcome than 1 or 2 has been lost. Such relative probabilities are critical to the notion of an expected value: the expected value of the program above is |avg [0, avg [1, 2]] = {-"\frac{3}{4}"-} /= 1 = avg [0, 1, 2]|. This makes list nondeterminism unsuitable for our purposes.

Using tree nondeterminism instead solves the problem: we introduce the type |ne_tree| of non-empty trees, building on |ne_list|: 
\begin{code}
  Inductive ne_tree (T: Set): Set :=
    | Leaf: T -> ne_tree T
    | Node: ne_list (ne_tree T) -> ne_tree T.

  Definition ret_ne_tree {A: Set}: A -> ne_tree A := Leaf.

  Fixpoint bind_ne_tree (A B: Set)
    (m: ne_tree A) (k: A -> ne_tree B): ne_tree B :=
      match m with
      | Leaf a => k a
      | Node ts => Node (ne_list.map (fun x => bind_ne_tree x k) ts)
      end.

  Definition M_ne_tree: Monad := Build_Monad ne_tree bind_ne_tree ret_ne_tree.

  Definition pick_ne_tree (T: Set): ne_list T -> M_ne_tree T
    := Node fcmp ne_list.map Leaf.
\end{code}
We use non-empty trees because we do not consider partial functions, and using potentially empty trees would complicate the definition of a tree's average value below. This is also why we used |ne_list| for |pick|.

With this monad and pick operation, the same program now produces the tree |Node [Leaf 0, Node [Leaf 1, Leaf 2]]|, which preserves the relative probabilities. The expected value now coincides with the weighted average of these trees:
\begin{code}
  Definition ne_tree.avg: ne_tree R -> R := ne_tree.fold id ne_list.avg.
\end{code}

Relative probabilities are also the reason we use an $n$-ary choice primitive rather than a binary one, because correctly emulating (that is, without skewing the relative probabilities) an $n$-ary choice by a sequence of binary choices is only possible when $n$ is a power of two.

To denote the expected value of a \emph{measure} of the output of a program, we define

\begin{code}
  Definition expec (T: Set) (f: T -> nat): ne_tree T -> R
    := ne_tree.avg fcmp ne_tree.map f.
\end{code}

Thus, given a program |P| of type |M_ne_tree (list bool)|, |expec length P| denotes the expected length of the result list, if we interpret values of type |M_ne_tree T| as nondeterministically computed values of type |T|.

The function |expec| gives rise to a host of identities, such as

\begin{align}
  0 &\le |expec f t| \nonumber \\
  |expec (fun x => f x + g x) t| &= |expec f t + expec g t| \nonumber \\
  |expec ((* c) fcmp f)| &= |(* c) fcmp expec f| \nonumber \\
  |(forall x `In` t -> f x <= g x) -> expec f t| &\le |expec g t| \nonumber \\
  |(forall x `In` t -> f x = c) -> expec f t| &= c \nonumber \\
  |(forall x `In` t -> f x = 0) <-> expec f t| &= 0 \nonumber \\
  |expec f (t >>= (ret fcmp g))| &= |expec (f fcmp g) t| \nonumber \\
  |expec (f fcmp g) t| &= |expec f (ne_tree.map g t)| \label{expec_bind_lem}
\end{align}

To form the monad with which we will instantiate |qs| for the main theorem, we now piggyback the additive monoid on |nat| onto |M_ne_tree| using |MMT|, and call the result |NDP| (for ``nondeterministically profiled''):
\begin{code}
  Definition M_NDP: Monad := MMT NatAddMonoid M_ne_tree.

  Definition cmp_NDP (x y: T): M_NDP bool := ret_ne_tree (1, cmp x y).

  Definition qs_NDP := qs M_NDP cmp_NDP (lift pick_ne_tree).
\end{code}
We can now denote the expected comparison count for a |qs_NDP| application by |expec cost (qs_NDP l)|, and will use this in our statement of the main theorem in the next section.

But before we do so, we define a slight refinement of |expec| that specifically observes the monoid component of computations in monads formed by transforming |M_ne_tree| using |MMT| (like |NDP|).
\begin{code}
  Definition monoid_expec (m: Monoid) (f: m -> nat) {A: Set}
    : (MonoidMonadTrans.M m NeTreeMonad A) -> R := expec (f fcmp fst).
\end{code}
Since |cost = fst|, we have |expec cost t = monoid_expec id t|.

In addition to all the identities |monoid_expec| inherits from |expec|, it has some of its own. One identity states that if one transforms |M_ne_tree| using a monoid |m|, then for a monoid homomorphism |h| from |m| to the additive monoid on |nat|, |monoid_expec h| distributes over |bind|, provided that the expected monoid value of the right hand side does not depend on the computed value of the left hand side:

\begin{code}
  monoid_expec_plus: forall (m: Monoid) (h: m -> NatAddMonoid),
    monoid_homo h -> forall (A B: Set)
    (f: MMT m NeTreeMonad A) (g: A -> MMT m NeTreeMonad B):
    (forall x y `In` f -> monoid_expec h (g (snd x)) = monoid_expec h (g (snd y))),
    monoid_expec h (f >>= g) =
      monoid_expec h f + monoid_expec h (g (snd (ne_tree.head f))).
\end{code}

Since |id| is a monoid homomorphism, |monoid_expec_plus| applies to |NDP| and |expec cost|. In section \ref{reduction}, we will use |monoid_expec_plus| with another monoid and homomorphism.

\section{The Statement}
\label{statement}

The last thing needed before the main theorem can be stated, is the notion of big-O complexity. We use a standard textbook definition, except that we incorporate a measure function parameter |m|:

\begin{code}
  Definition bigO (X: Set) (m: X -> nat) (f: X -> R) (g: nat -> R): Prop
      := exists c n, forall x, n <= m x -> f x <= c * g (m x).

  Notation {-"``"-}over m,{-"\ "-} f =O( g ){-"\text{''''}"-} := bigO m f g.
\end{code}
We now state the main theorem.
\begin{code}
  Theorem qs_avg: over length, expec cost fcmp qs_NDP =O(fun n => INR (n * log2ceil n)).
\end{code}
Thanks to the property discussed at the start of the previous section, % perhaps too far back
|qs_avg| follows as a corollary from the stronger statement
\begin{code}
  qs_expec_cost: forall l, expec cost (qs_NDP l) <= 2 * length l * (1 + log2ceil (length l)),
\end{code}
the proof of which is described in the next two sections.

\section{Reduction to Pairwise Comparison Counts}
\label{reduction}

As described in the introduction, the key ingredient in the proof is a lemma giving a formula for the expected comparison count for individual pairs of input list elements, indexed a certain way. More specifically, if $X \equiv X_{I_0} \ldots X_{I_{n-1}}$ is the input list, with $I$ a permutation of |[0 ... n-1]| such that $X_0 \ldots X_n$ is sorted, then the expected comparison count for any $X_i$ and $X_j$ with $i < j$ is at most $2 / (1 + j - i)$. In other words, the expected comparison count for two input list elements is bounded by a simple function of the number of list elements that separate the two in the sort order. We prove this fact in the next section, but first show how |qs_expec_cost| follows from it.

Combined with the observation that the total expected comparison count ought to equal the sum of the expected comparison count for each individual pair of input elements, the property described above suggests breaking up the inequality into
\begin{code}
  expec cost (qs_NDP l) <= {-"\displaystyle{\sum_{(i, j) \in \Varid{IJ}}}"-} ecc i j <= 2 * length l * (1 + log2ceil (length l)),
\end{code}
where $\Varid{IJ} := \{ (i, j) \in [0, |length l|) \mid i < j \}$, and |ecc i j := 2 / (1 + j - i)|.

The right inequality is a strictly numerical affair, requiring a bit of analysis involving the harmonic series. As stated before, this part of the proof 
    was 
%%% could be 
fairly directly transcribed from the paper proof, with few complications and additions, and so we will not discuss it.

The left inequality is the challenging one. To bring it closer to the index summation, we first write |l| on the left side as |map (nth (sort l)) li|, where |sort| may be any sorting function (including |qs| itself), and where |li| is a permutation of |[0...n-1]| such that |map (nth (sort l)) li = l| (such an |li| can easily be proven to exist).

Next, we introduce a specialized monad and comparison operation that go one step further in focusing specifically on these indices.
\begin{code}
  Definition Monoid_U: Monoid := (list (nat prod nat), nil, ++).

  Definition U: Monad := MonoidMonadTrans.M Monoid_U NeTreeMonad.

  Definition lookup_cmp (x y: nat): comparison :=
    cmp (nth (sort l) x) (nth (sort l) y).

  Definition unordered_nat_pair (x y: nat): nat prod nat :=
    if x <= y then (x, y) else (y, x).

  Definition cmp_U (x y: nat): U comparison :=
    ret (unordered_nat_pair x y :: nil, lookup_cmp x y).

  Definition qs_U: list nat -> list nat := qs U cmp_U pick_U.
\end{code}
|qs_U| operates directly on lists of indices into |sort l|. Comparison of indices is defined by comparison of the |T| values they denote in |sort l|. Furthermore, rather than producing a grand total comparison count the way |NDP| does, |U| records every pair of indices compared, by using |MMT| with |Monoid_U|, the free monoid over |nat prod nat| pairs, instead of the additive monoid on |nat| we used up until now.

We now rewrite
\begin{code}expec cost (qs_NDP (map (nth (sort l)) li))
  = monoid_expec length (qs_U li) = expec (length fcmp fst) (qs_U li).
\end{code}
The first equality expresses that the expected number of comparisons counted by |NDP| is equal to the expected length of the list of comparisons recorded by |U|. In the formalisation, this is a separate lemma proved by induction on |qs|. The second equality merely unfolds the definition of |monoid_expec|.

After rewriting with identity \ref{expec_bind_lem} in section \ref{nondetexpec} on page \pageref{expec_bind_lem}, the goal becomes
\begin{code}
  expec length (ne_tree.map fst (qs_U li)) <= {-"\displaystyle{\sum_{(i, j) \in \Varid{IJ}}}"-} ecc i j.
\end{code}
We now invoke another lemma which bounds a nondeterministically computed list's expected length by the expected number of occurrences of specific values in that list. More specifically, it states that

\begin{code}
  forall (X: Set) (fr: X -> R) (q: list X) (t: ne_tree (list X)),
    (forall x `In` q, expec (count x) t <= fr x) ->
    (forall x `NotIn` q, expec (count x) t = 0) -> expec length t <= {-"\displaystyle{\sum_{\Varid{x} \in \Varid{q}}\Varid{fr}\ \Varid{x}}"-}.
\end{code}
We end up with two subgoals, the first of which is \begin{code}forall (i, j) `NotIn` IJ, expec (count (i, j)) (ne_tree.map fst (qs_U li)) = 0.\end{code} Rewriting this using identity \ref{expec_bind_lem} from section \ref{nondetexpec} in reverse, then rewriting the |expec| as a |monoid_expec|, and then generalizing the premise, results in
%%% this needs a label!!! (\label{monoid_expec_zero_lem})
\begin{align}
  |forall i j li, (i `NotIn` li \/ j `NotIn` li) -> monoid_expec (count (i, j)) (U.qs li) = 0| \label{monoid_expec_zero_lem}
\end{align}
which can be shown by induction on |qs|, although we will not do so in this paper. We \emph{will} use this property again in the next section.

The second subgoal, expressed with |monoid_expec|, becomes
%%% this needs a label!!! (\label{monoid_expec_IJ_lem})
\begin{align}
  |forall (i, j) `In` IJ, monoid_expec (count (i, j)) (qs_U li) <= ecc i j| \label{monoid_expec_IJ_lem}
\end{align}
which corresponds exactly to the property described at the beginning of this section. We prove it in the next section.

\section{Finishing the Proof}
\label{finishing}

Again, the proof of (\ref{monoid_expec_IJ_lem}) 
is by induction on |qs|. But to get a better induction hypothesis, we drop the |(i, j) `In` IJ| premise (because as was shown in the last section, the statement is also true if |(i, j) `NotIn` IJ|), and add a premise saying |li| is a permutation of a contiguous sequence of indices.
\begin{code}
  forall i j, i < j -> forall (li: list nat) (b: nat), Permutation [b ... b + length li - 1] li ->
    monoid_expec (count (i, j)) (qs_U li) <= ecc i j.
\end{code}
In the base case, |li| is |nil|, and the left side of the inequality reduces to 0. In the recursive case, |qs| unfolds:
\begin{code}
monoid_expec (count (i, j)) (
    pi <- pick [0 ... n-1];
    let pivot := nth li pi in
    part <- partition_U pivot (remove li pi);
    lower <- qs_U (part Lt);
    upper <- qs_U (part Gt);
    ret (lower ++ pivot :: part Eq ++ upper)
  ) <= ecc i j.
\end{code}
Since |cmp_U| is deterministic, |partition_U| is as well. Furthermore, since
we know exactly what monadic effects |partition_U| has, we can split those
effects off and revert to simple effect-free |filter| passes. Finally, we rewrite using the following |monoid_expec| identity:
\begin{code}
  monoid_expec f (pick l >>= m) = avg (map (monoid_expec f fcmp m) l).
\end{code}
This way, the goal ends up in a form using less monadic indirection:
\begin{code}
avg (map (monoid_expec (count (i, j)) fcmp (fun pi =>
    let pivot := nth li pi in
    let rest := remove li pi in
    let flt := fun c => filter ((= c) fcmp lookup_cmp pivot) rest in
    ne_tree.map (map_fst (++ map (unordered_nat_pair pivot) rest)) (
      lower <- qs_U (flt Lt);
      upper <- qs_U (flt Gt);
      ret (lower ++ pivot :: flt Eq ++ upper)
    ))) [0...n-1]) <= ecc i j.
\end{code}
Here, |map_fst| applies a function to a pair's first component.

We now distinguish between five different cases that can occur for the nondeterministically picked |pivot| (which, because we are in the |U| monad, is an index). It can either be less than |i|, equal to |i|, between |i| and |j|, equal to |j|, or greater than |j|. Each case occurs a certain number of times, and has an associated expected number of |(i, j)| comparisons (coming either from the |map_fst| term representing the |partition| pass, or from the two recursive |qs_U| calls). To represent this split, we first rewrite the right side of the inequality to\[\frac{\Varid{ecc}\ i\ j * (i - b) + 1 + 0 + 1 + \Varid{ecc}\ i\ j * (b + n - j)}{n}\ .\]
This form reflects the facts that
\begin{itemize}
\item the case where |pivot| is less than |i| occurs |i - b| times, and in each instance, the expected number of |(i, j)| comparisons is no more than |ecc i j|;
\item the case where the |pivot| is equal to |i| occurs once, and in this case no more than a single |(i, j)| comparison is expected;
\item in the case where |pivot| lies between |i| and |j|, the number of expected |(i, j)| comparisons is 0, and consequently it does not matter how many times this case occurs;
\item the case where the |pivot| is equal to |j| occurs once, and in this case no more than a single |(i, j)| comparison is expected;
\item the case where the |pivot| is greater than |j| occurs |b + n - j| times, and in each instance, the expected number of |(i, j)| comparisons is no more than |ecc i j|.
\end{itemize}
With the right side of the inequality in this form, we unfold the |avg| application on the left into |sum (...) / n|, and then cancel the division by |n| on both sides. Next, to actually realize the split, we apply a specialized lemma stating that
\begin{code}
   forall b i j X f n (li: list nat)
     (g: [0...n-1] -> U X), Permutation [b ... b + length li - 1] li ->
    b <= i < j < b + S n -> forall ca cb, 0 <= ca -> 0 <= cb ->
    (forall pi, nth li pi < i -> expec f (g pi) <= ca) ->
    (forall pi, nth li pi = i -> expec f (g pi) <= cb) ->
    (forall pi, i < nth li pi < j -> expec f (g pi) = 0) ->
    (forall pi, nth li pi = j -> expec f (g pi) <= cb) ->
    (forall pi, j < nth li pi -> expec f (g pi) <= ca) ->
      sum (map (expec f fcmp g) [0..n]) <=
        ca * (i - b) + cb + 0 + cb + ca * (b + n - j).
\end{code}

Five subgoals remain after applying the above lemma---one for each of the listed cases. The first one reads
\begin{code}
  forall pi,
    let pivot := nth li pi in
    let rest := remove li pi in
      pivot < i ->
      monoid_expec (count (i, j))
        (ne_tree.map (map_fst (++ map (unordered_nat_pair pivot) rest)) (
          foo <- qs_U (filter ((= Lt) fcmp lookup_cmp pivot) rest);
          bar <- qs_U (filter ((= Gt) fcmp lookup_cmp pivot) rest);
          ret (foo ++ (pivot :: filter ((= Gt) fcmp lookup_cmp pivot) rest) ++ bar)))
        <= ecc i j.
  \end{code}
Since |count (i, j)| is a monoid homomorphism, we may rewrite using another lemma saying that
\begin{code}
  forall (m: Monoid) (h: m -> NatAddMonoid), monoid_homo h ->
  forall (g: m) (A: Set) (t: MMT m NeTreeMonad A),
    monoid_expec h (ne_tree.map (map_fst (monoid_mult m g)) t) =
    INR (h g) + monoid_expec h t.
\end{code}
This leaves
\begin{code}
    count (i, j) (map (unordered_nat_pair pivot) rest) +
    monoid_expec (count (i, j))
      (foo <- qs_U (filter ((= Lt) fcmp lookup_cmp pivot) rest);
      bar <- qs_U (filter ((= Gt) fcmp lookup_cmp pivot) rest);
      ret  (foo ++ (nth v pi :: filter ((= Eq) fcmp lookup_cmp pivot) rest) ++ bar))
    <= ecc i j.
\end{code}
From |pivot < i| and |i < j|, we have |pivot < j|. Since each of the comparisons in |map (unordered_nat_pair pivot) rest| involves the pivot element, it follows that none of them can represent comparisons between |i| and |j|. Hence, the first term vanishes. Furthermore, |monoid_expec_plus| lets us distribute |monoid_expec| over the |bind| applications. Since the |ret| term does not produce any comparisons either (by definition), its |monoid_expec| term vanishes, too. What remains are the two recursive calls:

\begin{code}
monoid_expec (count (i, j)) (qs_U (filter ((= Lt) fcmp lookup_cmp pivot) rest)) +
monoid_expec (count (i, j)) (qs_U (filter ((= Gt) fcmp lookup_cmp pivot) rest))
  <= ecc i j.
\end{code}
All indices in the first filtered list denote elements that are less than the element denoted by the pivot. Since the former precede the latter in |sort l|, it must be the case that these indices are all less than |pivot|. And since |pivot < i|, it follows that the first |qs_U| term will produce no |(i, j)| comparisons (using property 
   (\ref{monoid_expec_zero_lem})  
shown at the end of the previous section). Hence, the first |monoid_expec| term vanishes as well, leaving
\begin{code}
monoid_expec (count (i, j))
  (qs_U (filter ((= Gt) fcmp lookup_cmp pivot) rest)) <= ecc i j.
\end{code}
We now compare |nth (sort l) i| with |nth (sort l) pivot|.
\begin{itemize}
\item If the two are equal, then |i| will not occur in the |filter| term, and so (again) no |(i, j)| comparisons are performed.
\item If |nth (sort l) i < nth (sort l) pivot|, then we must have |i < pivot|, contradicting the assumption that |pivot < i|.
\item If |nth (sort l) i > nth (sort l) pivot|, then we apply the induction hypothesis. For this, it must be shown that filtering the list of indices preserves contiguity, which follows from the fact that the indices share the order of the elements they denote in |sort l|.
\end{itemize}
This concludes the case where |pivot < i|. The case where |j < pivot| is symmetric. The other three cases use similar arguments.

The proof is now complete.

\section{Final Remarks}
\label{conclusion}

In the interest of brevity, we have omitted lots of detail and various lemmas in the description of the proof. Still, the parts shown are reasonably faithful to the actual formalisation, with two notable exceptions.

First, we have pretended to have used ordinary natural numbers as indices into ordinary lists, completely ignoring issues of index validity that could not be ignored in the actual formalisation. There, we use vectors (lists whose size is part of their type) and bounded natural numbers in many places instead. Using these substantially reduces the amount of |i < length l| proofs that need to be produced, converted, and passed around, but this solution is still far from painless.

Second, using the |Program| facility to deal with Quicksort's non-structural recursion is not completely as trivial as we made it out to be. Since the recursive calls are nested in lambda abstractions passed to the |bind| operation of an unspecified monad, the relation between their arguments and the function's parameters is not locally known, resulting in unprovable proof obligations. To make these provable, we $\Sigma$-decorated the types of |filter| and |partition| in the actual formalisation with modest length guarantees.
% \item Using the induction principles corresponding to the definitions produced by |Program| for a function already defined with two layers of monadicity quickly became unwieldy. To deal with this, we defined a series of specialized induction principles for specific instances of |qs|, which put the recursive proof obligation in as simple terms as possible, with as little monadic indirection as possible, using among other things the observations regarding determinism of the comparison operation, as described in section \ref{finishing}.

The formalised development successfully adopted from the original proof the idea of using a nondeterministic version of the algorithm to make the $O(n \log n)$ bound hold for any input list, the idea of taking an order-indexed perspective to reduce the problem to a sum-of-expected-comparison-counts, and the use of the standard bound for harmonic series for the strictly numerical part. However, for the actual reduction and the derivation of the formula for the expected comparison count, the intuitive arguments essentially had to be reworked from scratch, building on the monadic representation of the algorithm and the various comparison counting/nondeterminism monads.

The shallow monadic embedding provides a simple but effective representation of the algorithm. Being parameterized on the monad used, it allows a single definition to be instantiated either with basic monads (like the identity monad or bare nondeterminism monads) to get a non-instrumented version suitable for extraction and correctness proofs, or with |MMT|-transformed monads to support complexity proofs. Furthermore, since the shallowness of the embedding lets us re-use all standard Coq data types and facilities, including the powerful |Program Fixpoint| command, the actual algorithm definition itself is reasonably clean.

We have shown that it is straightforward to give a fully formal treatment in type theory of a classical result in complexity theory. This clearly shows the utility and applicability of the general monadic 
%%%framework for 
approach to worst- and average-case complexity that we have developed. 

\bibliographystyle{plain}
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{paper}

\end{document}
